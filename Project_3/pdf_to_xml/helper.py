import re
from lxml import etree


def clean_page_text(text: str, book_info: dict[str, str]) -> str:
    """L√†m s·∫°ch text t·ª´ PDF, lo·∫°i b·ªè header/footer v√† text kh√¥ng c·∫ßn thi·∫øt"""
    lines = text.splitlines()
    cleaned = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
        # B·ªè s·ªë trang
        if re.match(r"^\d+/\d+$", line):
            continue
        # B·ªè title l·∫∑p l·∫°i
        if book_info["title"] in line or book_info["author"] in line:
            continue
        # B·ªè footer
        line = re.sub(r"https://thuviensach.vn", "", line, flags=re.IGNORECASE)
        line = re.sub(r"Trang\s+\d+/\d+\s+http://\S+", "", line, flags=re.IGNORECASE)
        # B·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
        line = re.sub(
            r"[^\w\s\.,!?;:()\-\"\'√†√°·∫£√£·∫°·∫ß·∫•·∫©·∫´·∫≠·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç·ªì·ªë·ªï·ªó·ªô·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒëƒê]",
            "",
            line,
        )
        if line.strip():
            cleaned.append(line)

    # G·ªôp l·∫°i v√† l√†m s·∫°ch kho·∫£ng tr·∫Øng
    return re.sub(r"\s+", " ", " ".join(cleaned)).strip()


def split_into_paragraphs_optimized(text, sentences_per_paragraph=4):
    """Chia text th√†nh c√°c ƒëo·∫°n vƒÉn v·ªõi 3-4 c√¢u m·ªói paragraph (optimized for memory)"""
    if not text.strip():
        return []

    # Chia th√†nh c√¢u (s·ª≠ d·ª•ng regex ƒë∆°n gi·∫£n h∆°n)
    sentences = re.split(r"(?<=[.!?])\s+", text)

    paragraphs = []
    current_paragraph = []

    for sentence in sentences:
        if not sentence.strip():
            continue

        current_paragraph.append(sentence.strip())

        # Khi ƒë·ªß 3-4 c√¢u, t·∫°o paragraph m·ªõi
        if len(current_paragraph) >= sentences_per_paragraph:
            paragraph_text = " ".join(current_paragraph)
            if paragraph_text.strip():
                paragraphs.append(paragraph_text)
            current_paragraph = []

    # Th√™m paragraph cu·ªëi c√πng n·∫øu c√≤n
    if current_paragraph:
        paragraph_text = " ".join(current_paragraph)
        if paragraph_text.strip():
            paragraphs.append(paragraph_text)

    # N·∫øu kh√¥ng chia ƒë∆∞·ª£c theo c√¢u, chia theo t·ª´ (fallback)
    if not paragraphs:
        words = text.split()
        if words:
            # Chia th√†nh chunks ~150-200 t·ª´
            chunk_size = 150
            for i in range(0, len(words), chunk_size):
                chunk = " ".join(words[i : i + chunk_size])
                if chunk.strip():
                    paragraphs.append(chunk)

    return paragraphs


def create_dtbook_structure(book_info: dict[str, str]):
    """T·∫°o c·∫•u tr√∫c DTBook chu·∫©n cho DAISY 3.0"""
    NS = {"dtb": "http://www.daisy.org/z3986/2005/dtbook/"}
    dtbook = etree.Element(
        "dtbook",
        nsmap={None: NS["dtb"]},
        version="2005-3",
        **{"{http://www.w3.org/XML/1998/namespace}lang": book_info["language"]},
    )

    head = etree.SubElement(dtbook, "head")

    meta_items = [
        ("dc:Title", book_info["title"]),
        ("dc:Creator", book_info["author"]),
        ("dc:Subject", book_info["subject"]),
        ("dc:Description", book_info["description"]),
        ("dc:Publisher", book_info["publisher"]),
        ("dc:Date", book_info["date"]),
        ("dc:Format", "DAISY 3.0"),
        ("dc:Identifier", book_info["identifier"]),
        ("dc:Language", book_info["language"]),
        # ("dtb:multimediaType", "textOnly"),
        # ("dtb:multimediaContent", "text"),
        # ("dtb:totalPageCount", book_info["total_page_count"]),
    ]

    for name, content in meta_items:
        etree.SubElement(head, "meta", name=name, content=content)

    return dtbook, head


def validate_dtbook(xml_path):
    """Ki·ªÉm tra DTBook file"""
    try:
        tree = etree.parse(xml_path)
        root = tree.getroot()

        print("\nüîç KI·ªÇM TRA FILE DTBOOK:")
        print(f"‚úÖ File XML h·ª£p l·ªá")

        ns = {"dtb": "http://www.daisy.org/z3986/2005/dtbook/"}

        # ƒê·∫øm elements quan tr·ªçng
        counts = {
            "level1": len(root.xpath(".//dtb:level1", namespaces=ns)),
            "p": len(root.xpath(".//dtb:p", namespaces=ns)),
            "pagenum": len(root.xpath(".//dtb:pagenum", namespaces=ns)),
        }

        print(f"üìä Th·ªëng k√™:")
        for elem, count in counts.items():
            print(f"   {elem}: {count}")

        # Li·ªát k√™ sections
        h1_elements = root.xpath(".//dtb:h1", namespaces=ns)
        print(f"\nüìñ C√ÅC SECTIONS ƒê∆Ø·ª¢C T·∫†O:")
        for i, h1 in enumerate(h1_elements, 1):
            text = h1.text or "Kh√¥ng c√≥ text"
            print(f"   {i}. {text}")

        return True

    except Exception as e:
        print(f"‚ùå L·ªói khi ki·ªÉm tra file: {e}")
        return False
